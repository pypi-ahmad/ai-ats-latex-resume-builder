import streamlit as st
import os
import subprocess
import re
from io import BytesIO
import fitz  # PyMuPDF
import docx
from PIL import Image
import numpy as np
from ddgs import DDGS

# Imports for LLMs
import ollama
from openai import OpenAI
import anthropic

try:
    from google import genai
    from google.genai import types
    GOOGLE_GENAI_AVAILABLE = True
except ImportError:
    GOOGLE_GENAI_AVAILABLE = False
    try:
        import google.genai as google_genai_old
        GOOGLE_GENAI_OLD_AVAILABLE = True
    except ImportError:
        GOOGLE_GENAI_OLD_AVAILABLE = False

# Conditional Import for PaddleOCR
try:
    from paddleocr import PaddleOCR
    PADDLE_AVAILABLE = True
except ImportError:
    PADDLE_AVAILABLE = False
except Exception:
    PADDLE_AVAILABLE = False

# --- Global Config ---
st.set_page_config(page_title="AI Resume Builder v2.0", layout="wide")

# --- Helper Functions ---

def clean_latex_code(text):
    """
    Cleans the LaTeX code generated by the LLM.
    
    Args:
        text (str): The raw text output from the LLM.
        
    Returns:
        str: The cleaned LaTeX code, stripped of markdown blocks and with basic ampersand escaping.
    """
    # Remove markdown code blocks if present
    text = re.sub(r'```latex', '', text, flags=re.IGNORECASE)
    text = re.sub(r'```', '', text)
    
    # Smart cleanup: Escape & in the body only (to avoid breaking preamble macros)
    # This prevents unescaped '&' in content (like "R&D") from breaking LaTeX tables
    if "\\begin{document}" in text:
        parts = text.split("\\begin{document}")
        preamble = parts[0]
        # Rejoin all parts after the first split in case \begin{document} appears multiple times (unlikely)
        body = "\\begin{document}".join(parts[1:]) 
        
        # Look for ampersands that are NOT preceded by a backslash
        body = re.sub(r'(?<!\\)&', r'\\&', body)
        
        text = preamble + "\\begin{document}" + body
        
    return text.strip()

def compile_latex(tex_filename):
    """
    Compiles a LaTeX file to PDF using pdflatex.
    
    Args:
        tex_filename (str): The path to the .tex file to compile.
        
    Returns:
        tuple: (bool, str) - (Success status, Output message or error log).
    """
    # Check if pdflatex is available in the system PATH
    try:
        subprocess.run(["pdflatex", "--version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    except FileNotFoundError:
        return False, "pdflatex not found. Please ensure MikTeX or TeX Live is installed and in your PATH."

    try:
        # Run pdflatex in non-stop mode to avoid hanging on errors
        result = subprocess.run(
            ["pdflatex", "-interaction=nonstopmode", tex_filename],
            capture_output=True,
            text=True,
            check=False
        )
        
        if result.returncode != 0:
            return False, result.stdout + "\n" + result.stderr
        
        return True, "Compilation successful."
    except Exception as e:
        return False, str(e)

def get_market_research(role):
    """
    Performs a DuckDuckGo search to find resume trends for a specific job role.
    
    Args:
        role (str): The job title to research (e.g., "Data Scientist").
        
    Returns:
        str: A summary of top search results or an error message.
    """
    if not role:
        return "No role specified."
    try:
        query = f"latest resume trends and keywords for {role} 2025"
        with DDGS() as ddgs:
            # Fetch top 3 results
            results = list(ddgs.text(query, max_results=3))
        
        summary = "\n".join([f"- {r['title']}: {r['body']}" for r in results])
        return summary
    except Exception as e:
        return f"Market research failed: {e}"

def get_ollama_models():
    """
    Fetches the list of available local Ollama models.
    
    Returns:
        list: A list of model names (str). Returns a default fallback list on failure.
    """
    try:
        models_info = ollama.list()
        # Handle different response formats (object vs dict) depending on library version
        if hasattr(models_info, 'models'):
            return [m.model for m in models_info.models]
        if 'models' in models_info:
            return [m['name'] for m in models_info['models']]
        return ["llama3", "mistral", "glm-4.7-flash"]
    except Exception:
        return ["llama3", "mistral", "glm-4.7-flash"] # Fallback

# --- Configuration (Sidebar) ---

def get_llm_config():
    """
    Renders the Sidebar configuration UI and returns the user settings.
    
    Returns:
        dict: A dictionary containing 'provider', 'model', 'api_key', 'base_url', 
              and vision-related settings.
    """
    st.sidebar.title("âš™ï¸ Configuration")
    
    # 1. Brain (LLM) Settings
    st.sidebar.subheader("Brain (LLM)")
    provider = st.sidebar.selectbox("LLM Provider", ["Ollama", "OpenAI", "Google Gemini", "Anthropic"])
    
    config = {"provider": provider}
    
    # Render fields specific to the selected provider
    if provider == "Ollama":
        models = get_ollama_models()
        config['model'] = st.sidebar.selectbox("Select Model", models)
        config['api_key'] = None
        config['base_url'] = None
    elif provider == "OpenAI":
        config['api_key'] = st.sidebar.text_input("OpenAI API Key", type="password")
        config['base_url'] = st.sidebar.text_input("Base URL", "https://api.openai.com/v1")
        config['model'] = st.sidebar.text_input("Model Name", "gpt-4o")
    elif provider == "Google Gemini":
        config['api_key'] = st.sidebar.text_input("Gemini API Key", type="password")
        config['model'] = "gemini-2.5-flash"
    elif provider == "Anthropic":
        config['api_key'] = st.sidebar.text_input("Anthropic API Key", type="password")
        config['model'] = "claude-4-5-sonnet-latest"

    # 2. Eyes (Vision/OCR) Settings
    st.sidebar.subheader("Eyes (Vision/OCR)")
    vision_options = ["Same as LLM", "PaddleOCR", "Google Gemini (Free Tier)", "Local Ollama Vision"]
    vision_provider = st.sidebar.selectbox("Vision Provider", vision_options)
    config['vision_provider'] = vision_provider
    
    # Credentials/Models for Vision if different from LLM
    if vision_provider == "Google Gemini (Free Tier)" and provider != "Google Gemini":
        config['vision_api_key'] = st.sidebar.text_input("Vision: Gemini API Key", type="password", key="vis_gemini")
    elif vision_provider == "Local Ollama Vision":
        ollama_models = get_ollama_models()
        config['vision_model'] = st.sidebar.selectbox("Select Vision Model", ollama_models, help="Select a vision-capable model (e.g., llava, moondream, deepseek-vl)")
    
    return config

# --- Universal Smart Parser ---

def smart_extract_text(input_data, is_file, config):
    """
    Extracts text from a file (PDF, DOCX, Image) or returns raw text.
    Implements a multi-layered approach:
    1. Text Extraction (fast, for native PDFs/Docs)
    2. Vision/OCR (fallback for scanned PDFs or images)
    
    Args:
        input_data: The file object or string to process.
        is_file (bool): True if input_data is a file object.
        config (dict): Configuration dictionary containing provider settings.
        
    Returns:
        str: The extracted text content.
    """
    if not is_file:
        return input_data # Return raw text directly

    uploaded_file = input_data
    file_type = uploaded_file.name.split('.')[-1].lower()
    text_content = ""
    
    # Reset file pointer to beginning
    uploaded_file.seek(0)
    file_bytes = uploaded_file.read()

    # --- Layer 1: Fast Text Extraction (Native) ---
    if file_type == 'pdf':
        try:
            doc = fitz.open(stream=file_bytes, filetype="pdf")
            for page in doc: text_content += page.get_text() + "\n"
            # If we got substantial text, assume it's not a scanned PDF
            if len(text_content.strip()) > 50: return text_content
        except: pass
    elif file_type == 'docx':
        try:
            doc = docx.Document(BytesIO(file_bytes))
            text_content = "\n".join([p.text for p in doc.paragraphs])
            if len(text_content.strip()) > 50: return text_content
        except: pass
    elif file_type == 'txt':
        return file_bytes.decode('utf-8', errors='ignore')

    # --- Layer 2: Image Conversion (for Scanned Docs) ---
    image = None
    if file_type == 'pdf':
        try:
            # Convert first page of PDF to image
            doc = fitz.open(stream=file_bytes, filetype="pdf")
            if len(doc) > 0:
                pix = doc[0].get_pixmap()
                image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        except: pass
    elif file_type in ['png', 'jpg', 'jpeg']:
        image = Image.open(BytesIO(file_bytes))

    if image is None: return "Failed to extract text or convert document to image."

    # --- Layer 3: Vision/OCR Extraction ---
    
    # Determine the active provider for vision tasks
    vis_prov = config['vision_provider']
    active_provider = config['provider'] if vis_prov == "Same as LLM" else vis_prov
    
    # Use vision key if available, else fallback to main key
    api_key = config.get('vision_api_key') or config.get('api_key')
    
    st.info(f"Scanned document detected. Engaging Vision Layer: {active_provider}")

    # 1. PaddleOCR (Local Offline OCR)
    if active_provider == "PaddleOCR":
        if PADDLE_AVAILABLE:
            try:
                ocr = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
                res = ocr.ocr(np.array(image), cls=True)
                # Join recognized text lines
                return "\n".join([line[1][0] for line in res[0]])
            except Exception as e: return f"PaddleOCR Error: {e}"
        else:
            return "PaddleOCR not installed or compatible."

    # 2. Google Gemini (Vision)
    if "Google Gemini" in active_provider:
        if not api_key: return "Vision Error: Missing Gemini API Key."
        try:
            if GOOGLE_GENAI_AVAILABLE:
                client = genai.Client(api_key=api_key)
                response = client.models.generate_content(
                    model='gemini-2.5-flash',
                    contents=["Transcribe this resume image exactly.", image]
                )
                return response.text
            elif GOOGLE_GENAI_OLD_AVAILABLE:
                google_genai_old.configure(api_key=api_key)
                model = google_genai_old.GenerativeModel('gemini-2.5-flash')
                response = model.generate_content(["Transcribe this resume image exactly.", image])
                return response.text
        except Exception as e: return f"Gemini Vision Error: {e}"

    # 3. OpenAI (Vision)
    if active_provider == "OpenAI":
        if not api_key: return "Vision Error: Missing OpenAI API Key."
        try:
            import base64
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            
            client = OpenAI(api_key=api_key, base_url=config.get('base_url'))
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": "Transcribe this resume image exactly."},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_str}"}}
                    ]}
                ]
            )
            return response.choices[0].message.content
        except Exception as e: return f"OpenAI Vision Error: {e}"

    # 4. Anthropic (Vision)
    if active_provider == "Anthropic":
        if not api_key: return "Vision Error: Missing Anthropic API Key."
        try:
            import base64
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            
            client = anthropic.Anthropic(api_key=api_key)
            response = client.messages.create(
                model="claude-3-5-sonnet-latest", max_tokens=4096,
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": "Transcribe this resume image exactly."},
                        {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": img_str}}
                    ]}
                ]
            )
            return response.content[0].text
        except Exception as e: return f"Anthropic Vision Error: {e}"

    # 5. Local Ollama Vision
    if active_provider == "Local Ollama Vision":
        try:
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_bytes = buffered.getvalue()
            
            vision_model = config.get('vision_model', 'llava') # Fallback to llava
            
            response = ollama.chat(
                model=vision_model,
                messages=[{
                    'role': 'user',
                    'content': 'Transcribe the text from this resume image exactly as it appears, preserving structure.',
                    'images': [img_bytes]
                }]
            )
            return response['message']['content']
        except Exception as e: return f"Ollama Vision Error: {e}"

    return "No suitable Vision Provider found (or DeepSeek-OCR placeholder selected)."


# --- Main Application Flow ---

def main():
    """
    Main execution logic for the Streamlit application.
    Handles UI rendering, state management, and workflow orchestration.
    """
    config = get_llm_config()
    st.title("ðŸš€ AI Resume Builder v2.0")

    # Session State Initialization - Preserves data across reruns
    if 'step' not in st.session_state: st.session_state.step = 1
    if 'extracted_text' not in st.session_state: st.session_state.extracted_text = ""
    if 'research' not in st.session_state: st.session_state.research = ""
    if 'target_role' not in st.session_state: st.session_state.target_role = ""
    if 'job_desc' not in st.session_state: st.session_state.job_desc = ""
    
    # State for Resume Generation Results
    if 'generated_success' not in st.session_state: st.session_state.generated_success = False
    if 'generated_pdf_data' not in st.session_state: st.session_state.generated_pdf_data = None
    if 'generated_tex_data' not in st.session_state: st.session_state.generated_tex_data = None
    if 'generation_error' not in st.session_state: st.session_state.generation_error = None

    # --- Step 1: Input & Analysis ---
    if st.session_state.step == 1:
        st.header("Step 1: Profile & Target")
        
        col1, col2 = st.columns(2)
        with col1:
            target_role = st.text_input("Target Role", value=st.session_state.target_role, placeholder="e.g. Senior DevOps Engineer")
        with col2:
            job_desc = st.text_area("Job Description (Optional)", value=st.session_state.job_desc, height=100)
            
        st.subheader("Input Source")
        input_method = st.radio("Choose Input Method", ["File Upload", "Raw Text"], horizontal=True)
        
        user_input = None
        is_file = False
        
        if input_method == "File Upload":
            user_input = st.file_uploader("Upload Resume (PDF/DOCX/Image)", type=['pdf', 'docx', 'png', 'jpg', 'txt'])
            is_file = True
        else:
            user_input = st.text_area("Paste Resume Text Here", height=200)
            is_file = False

        if st.button("Analyze Profile", type="primary"):
            if not user_input or not target_role:
                st.error("Please provide both a Resume and a Target Role.")
            else:
                with st.spinner("Analyzing Document & Market Trends..."):
                    # 1. Extraction: Parse the uploaded file or text
                    extracted_text = smart_extract_text(user_input, is_file, config)
                    st.session_state.extracted_text = extracted_text
                    
                    # 2. Market Research: Fetch trends for the role
                    research = get_market_research(target_role)
                    st.session_state.research = research
                    
                    # 3. Save State & Advance to Step 2
                    st.session_state.target_role = target_role
                    st.session_state.job_desc = job_desc
                    st.session_state.generated_success = False # Reset prev generation
                    st.session_state.step = 2
                    st.rerun()

    # --- Step 2: Review & Generate ---
    elif st.session_state.step == 2:
        st.header("Step 2: Review & Draft")
        
        # Navigation
        if st.button("â¬… Back to Input"):
            st.session_state.step = 1
            st.rerun()

        # Editable Fields
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Extracted Profile")
            st.caption("Edit this text to correct any parsing errors.")
            edited_profile = st.text_area("Profile Data", value=st.session_state.extracted_text, height=400, key="edit_profile")
            st.session_state.extracted_text = edited_profile
            
        with col2:
            st.subheader("Market Insights")
            st.caption("Key trends found for your role.")
            edited_research = st.text_area("Market Trends", value=st.session_state.research, height=400, key="edit_research")
            st.session_state.research = edited_research

        st.markdown("---")
        
        # Generation Button
        if st.button("Draft Resume (Generate LaTeX)", type="primary", use_container_width=True):
            with st.spinner("Generating LaTeX Resume... (This may take a minute)"):
                try:
                    # Load Template
                    try:
                        with open("template.tex", "r") as f:
                            template_content = f.read()
                    except FileNotFoundError:
                        st.error("template.tex not found!")
                        st.stop()

                    # Construct Prompt for the LLM
                    system_prompt = "You are an expert resume writer and LaTeX developer."
                    user_prompt = f"""
                    Generate a complete LaTeX resume based on the data below.
                    
                    **1. User Profile:**
                    {st.session_state.extracted_text}
                    
                    **2. Target Role:** {st.session_state.target_role}
                    
                    **3. Job Description:**
                    {st.session_state.job_desc}
                    
                    **4. Market Trends:**
                    {st.session_state.research}
                    
                    **5. LaTeX Template:**
                    {template_content}
                    
                    **INSTRUCTIONS:**
                    - Fill the template with the user's info, tailored to the role/JD.
                    - Use keywords from Market Trends.
                    - **CRITICAL**: Escape special LaTeX chars (e.g. use \& for &, \% for %).
                    - Output ONLY valid LaTeX code.
                    - Do NOT remove packages/preamble.
                    """
                    
                    generated_latex = ""
                    prov = config['provider']
                    
                    # Call the selected LLM Provider
                    if prov == "Ollama":
                        resp = ollama.chat(model=config['model'], messages=[
                            {'role': 'system', 'content': system_prompt},
                            {'role': 'user', 'content': user_prompt}
                        ])
                        generated_latex = resp['message']['content']
                        
                    elif prov == "OpenAI":
                        if not config['api_key']: raise ValueError("Missing OpenAI API Key")
                        client = OpenAI(api_key=config['api_key'], base_url=config['base_url'])
                        resp = client.chat.completions.create(
                            model=config['model'],
                            messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}]
                        )
                        generated_latex = resp.choices[0].message.content
                        
                    elif prov == "Google Gemini":
                        if not config['api_key']: raise ValueError("Missing Gemini API Key")
                        if GOOGLE_GENAI_AVAILABLE:
                            client = genai.Client(api_key=config['api_key'])
                            resp = client.models.generate_content(model=config['model'], contents=system_prompt + "\n" + user_prompt)
                            generated_latex = resp.text
                        elif GOOGLE_GENAI_OLD_AVAILABLE:
                            google_genai_old.configure(api_key=config['api_key'])
                            model = google_genai_old.GenerativeModel(config['model'])
                            resp = model.generate_content(system_prompt + "\n" + user_prompt)
                            generated_latex = resp.text
                            
                    elif prov == "Anthropic":
                        if not config['api_key']: raise ValueError("Missing Anthropic API Key")
                        client = anthropic.Anthropic(api_key=config['api_key'])
                        resp = client.messages.create(
                            model=config['model'], max_tokens=4096,
                            messages=[{'role': 'user', 'content': system_prompt + "\n\n" + user_prompt}]
                        )
                        generated_latex = resp.content[0].text

                    # Clean the output code
                    generated_latex = clean_latex_code(generated_latex)
                    
                    # Save to file
                    with open("generated_resume.tex", "w", encoding="utf-8") as f:
                        f.write(generated_latex)
                        
                    # Compile to PDF
                    success, msg = compile_latex("generated_resume.tex")
                    
                    if success:
                        st.session_state.generated_success = True
                        st.session_state.generation_error = None
                        
                        # Read binary data into session state for persistent download buttons
                        with open("generated_resume.pdf", "rb") as f:
                            st.session_state.generated_pdf_data = f.read()
                        with open("generated_resume.tex", "rb") as f:
                            st.session_state.generated_tex_data = f.read()
                    else:
                        st.session_state.generated_success = False
                        st.session_state.generation_error = msg
                            
                except Exception as e:
                    st.session_state.generated_success = False
                    st.session_state.generation_error = f"Generation Error: {e}"

        # --- Display Results (Persistent) ---
        if st.session_state.generated_success:
            st.success("Resume Generated Successfully!")
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.download_button(
                    label="Download PDF",
                    data=st.session_state.generated_pdf_data,
                    file_name="resume.pdf",
                    mime="application/pdf",
                    use_container_width=True
                )
            with col_b:
                st.download_button(
                    label="Download Source",
                    data=st.session_state.generated_tex_data,
                    file_name="resume.tex",
                    mime="text/plain",
                    use_container_width=True
                )
        
        elif st.session_state.generation_error:
            st.error("Compilation Failed")
            with st.expander("Error Log"):
                st.code(st.session_state.generation_error)
            # Allow debug download of source even if compilation failed
            with open("generated_resume.tex", "rb") as f:
                st.download_button("Download Source (Debug)", f, "resume.tex", "text/plain")

if __name__ == "__main__":
    main()
